common:
  disable_tqdm: true # set true when using nohup
  seed: 0

path:
  checkpoint: "models/s5-hubert-dino"
  quantizer1: "models/s5-hubert-dino/quantizer1.npy"
  quantizer2: "models/s5-hubert-dino/quantizer2.npy"
  segment_dir: "segments/s5-hubert-dino"
  result: "results/speech2unit/s5-hubert-dino.json"

dataset:
  root: "data" # ${root}/LibriSpeech/train-clean-100, train-clean-360, ...
  lh_dir: "data/libriheavy" # ${lh_dir}/[small|medium|large]
  download: false
  max_sample_size: 80080 # 5 seconds
  train_file: "data/librispeech_train.txt"
  dev_file: "src/sdhubert/files/librispeech_val.txt"
  test_file: "src/sdhubert/files/librispeech_test.txt"
  dev_alignment: "src/sdhubert/files/librispeech_syllable_val.json"
  test_alignment: "src/sdhubert/files/librispeech_syllable_test.json"
  perturb: true

model:
  model_type: "s5hubert_dino"
  model_name_or_path: "facebook/hubert-base-ls960"
  init_last_layer: 3
  head_out_size: 2048
  head_hidden_size: 2048
  head_bottleneck_size: 256
  teacher_temp: 0.05
  student_temp: 0.2
  center_momentum: 0.9
  ema_decay: 0.999
  segmentation_layer: 8

training_args:
  output_dir: "models/s5-hubert-dino"
  per_device_train_batch_size: 216
  learning_rate: 0.0001
  weight_decay: 0.01
  max_grad_norm: 0.5
  max_steps: 13000
  lr_scheduler_type: "warmup_stable_decay"
  lr_scheduler_kwargs: {"num_decay_steps": 5200, "min_lr_ratio": 0.1, "decay_type": "linear"}
  warmup_steps: 2600
  logging_steps: 10
  save_steps: 1300
  save_total_limit: null
  bf16: true
  eval_steps: 1300
  dataloader_num_workers: 6
  remove_unused_columns: false
  optim: "adamw_torch_fused"
  ddp_find_unused_parameters: true
  resume_from_checkpoint: null

quantizer:
  n_clusters1: 24576
  n_clusters2: 16384
  niter: 100
  nredo: 5
  verbose: true
  random_state: ${common.seed}
  gpu: true
  min_points_per_centroid: 1
  max_points_per_centroid: null

mincut:
  sec_per_frame: 0.02
  sec_per_syllable: 0.15
  merge_threshold: 0.6
  min_duration: 3
  max_duration: 35
  num_workers: 2
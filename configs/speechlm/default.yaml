dataset:
  wav_dir: ""
  ext_audio: ".flac"

  train: ""
  swuggy: "ryota-komatsu/swuggy-s5-hubert-8192units"  # lexical
  sblimp: "ryota-komatsu/sblimp-s5-hubert-8192units"  # syntactic

  APP_DIR: "data/zr-data"
  result_dir: "results/speechlm"

dataloader:
  batch_size_per_device: 1000  # effective batch size (tokens) = dataset.units_per_sample * batch_size_per_device * #GPUs

model:
  name: ""
  path: "models/speechlm"

  lora:
    r: 32
    lora_alpha: 32
    target_modules: "all-linear"

optim:
  epoch: 24
  warmup_steps: 100
  lr: 0.0005
  lr_min: 0.00005
  beta1: 0.9
  beta2: 0.98
  max_norm: 0.1
  summary_interval: 100
  validation_save_interval: 10000
  total_steps: 200000

speech2unit:
  model_name_or_path: "ryota-komatsu/s5-hubert"
  num_workers: 4
  batch_size: 4   # work with a single 24GB VRAM GPU
dataset:
  ll_dir: "data/librilight"
  lh_dir: "data/libriheavy"
  ext_audio: ".flac"
  manifest_prefix: "data/manifest"

  train: "pkufool/libriheavy_long"
  name: "ryota-komatsu/s5-hubert"

  HF_HOME: "~/.cache/huggingface"
  APP_DIR: "data/zr-data"
  tSC_DIR: "data/tSC"

model:
  name: ""

training_args:
  output_dir: "models/speechlm"
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  learning_rate: 0.0005
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.98
  max_grad_norm: 0.5
  num_train_epochs: 24
  max_steps: 200000
  lr_scheduler_type: "cosine_with_min_lr"
  lr_scheduler_kwargs: {"min_lr": 0.00005}
  warmup_steps: 10000
  save_steps: 5000
  save_total_limit: 2
  bf16: true
  eval_steps: 5000
  remove_unused_columns: false
  fsdp: "shard_grad_op"
  skip_memory_metrics: true
  resume_from_checkpoint: null

speech2unit:
  model_name_or_path: "ryota-komatsu/s5-hubert"
  batch_size: 4   # work with a single 24GB VRAM GPU
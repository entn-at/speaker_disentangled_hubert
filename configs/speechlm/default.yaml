dataset:
  ll_dir: "data/librilight"
  lh_dir: "data/libriheavy"
  ext_audio: ".flac"
  manifest_prefix: "data/manifest"

  train: "pkufool/libriheavy_long"
  name: "ryota-komatsu/s5-hubert"

  HF_HOME: "~/.cache/huggingface"
  APP_DIR: "data/zr-data"
  tSC_DIR: "data/tSC"

model:
  name: "meta-llama/Llama-3.2-3B"
  mean_resizing: false
  defrost_steps: 20000

training_args:
  output_dir: "models/speechlm"
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 2
  learning_rate: 0.0005
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.98
  max_grad_norm: 0.5
  max_steps: 200000
  lr_scheduler_type: "cosine_with_min_lr"
  lr_scheduler_kwargs: {"min_lr": 0.00005}
  warmup_steps: 100
  save_steps: 1000
  save_total_limit: 2
  bf16: true
  eval_steps: 5000
  remove_unused_columns: false
  ddp_find_unused_parameters: false
  resume_from_checkpoint: null

speech2unit:
  model_name_or_path: "ryota-komatsu/s5-hubert"
  batch_size: 4   # work with a single 24GB VRAM GPU
  vocab_size: 16384
dataset:
  ll_dir: "data/librilight"
  lh_dir: "data/libriheavy"
  ext_audio: ".flac"
  manifest_prefix: "data/manifest"

  train: "pkufool/libriheavy_long"
  name: "ryota-komatsu/s5-hubert"

  HF_HOME: "~/.cache/huggingface"
  APP_DIR: "data/zr-data"
  tSC_DIR: "data/tSC"

  result_dir: "results/speechlm"

dataloader:
  batch_size_per_device: 1000  # effective batch size (tokens) = dataset.units_per_sample * batch_size_per_device * #GPUs

model:
  name: ""
  path: "models/speechlm"

optim:
  epoch: 24
  warmup_steps: 100
  lr: 0.0005
  lr_min: 0.00005
  beta1: 0.9
  beta2: 0.98
  max_norm: 0.5
  summary_interval: 100
  validation_save_interval: 10000
  total_steps: 200000

speech2unit:
  model_name_or_path: "ryota-komatsu/s5-hubert"
  batch_size: 4   # work with a single 24GB VRAM GPU